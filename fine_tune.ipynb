{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S67uwxlYL__i"
   },
   "source": [
    "# Finetuning Faster RCNN for Door Detection\n",
    "\n",
    "In this notebook, the [Faster RCNN](https://arxiv.org/abs/1506.01497) model is re-trained on the CubiCasa5k dataset to detect door bounding boxes. An intensive use of [PyTorch](https://pytorch.org/)'s functionalities has been used. Among other things, I used a pre-trained network and fine-tune a network with new data.\n",
    "\n",
    "**Note:** The Notbook was developed on a Linux environment and using CUDA 12.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version # confirm the version is 3.10.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJlScJvmUALl"
   },
   "source": [
    "Set the current working directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ybit1kj9T1PZ"
   },
   "outputs": [],
   "source": [
    "curren_dir = '/home/hudab/door_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk0I6WYpqOY0"
   },
   "source": [
    "Import all the libraries used.\n",
    "Please note to set the system path to the exercise folder correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j-KgAi3Fif9S"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('') # make sure path is correct here\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import random\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfY-7hMUoI55"
   },
   "source": [
    "## 1. Detection with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVEe1xomftEK"
   },
   "source": [
    "Selecting the device (CPU or GPU) on which to run the infernce and training.\n",
    "This notebook is set to use the GPU under Google Colab.\n",
    "\n",
    "Download the pre-trained **Faster R-CNN** with ResNet50 Backbone, on the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "True\n",
      ":)\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) #current version\n",
    "print(torch.cuda.is_available()) #check if GPU is available\n",
    "print(\":)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFERCLWbjafV",
    "outputId": "a474fdaf-509b-4b6d-a970-902695ef1402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use the following device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"We use the following device: \", device)\n",
    "# load a model; pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1toaKkykf_Q-"
   },
   "source": [
    "Transformation of a sample input for infernce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gwR_AkXllkLm"
   },
   "outputs": [],
   "source": [
    "def img_transform(img):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "  img /= 255.0\n",
    "  img = torch.from_numpy(img).permute(2,0,1)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IodDSdBqOY2"
   },
   "source": [
    "  Inference of a single input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "um6C1vDlkOfb"
   },
   "outputs": [],
   "source": [
    "def inference(img, model, detection_threshold=0.00):\n",
    "  '''\n",
    "  Infernece of a single input image\n",
    "\n",
    "  inputs:\n",
    "    img: input-image as torch.tensor (shape: [C, H, W])\n",
    "    model: model for infernce (torch.nn.Module)\n",
    "    detection_threshold: Confidence-threshold for NMS (default=0.7)\n",
    "\n",
    "  returns:\n",
    "    boxes: bounding boxes (Format [N, 4] => N times [xmin, ymin, xmax, ymax])\n",
    "    labels: class-prediction (Format [N] => N times an number between 0 and _num_classes-1)\n",
    "    scores: confidence-score (Format [N] => N times confidence-score between 0 and 1)\n",
    "  '''\n",
    "  model.eval()\n",
    "\n",
    "  img = img.to(device)\n",
    "  outputs = model([img])\n",
    "  # print(outputs)\n",
    "  \n",
    "  boxes = outputs[0]['boxes'].data.cpu().numpy()\n",
    "  scores = outputs[0]['scores'].data.cpu().numpy()\n",
    "  labels = outputs[0]['labels'].data.cpu().numpy()\n",
    "\n",
    "  boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "  labels = labels[scores >= detection_threshold]\n",
    "  scores = scores[scores >= detection_threshold]\n",
    "\n",
    "  return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzgtASZDlh7l"
   },
   "source": [
    "Function that draws the BBoxes, scores, and labels on the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5q7JEhaurymN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_image(img, boxes, scores, labels, dataset, save_path=None):\n",
    "  '''\n",
    "  Function that draws the BBoxes, scores, and labels on the image.\n",
    "\n",
    "  inputs:\n",
    "    img: input-image as numpy.array (shape: [H, W, C])\n",
    "    boxes: list of bounding boxes (Format [N, 4] => N times [xmin, ymin, xmax, ymax])\n",
    "    scores: list of conf-scores (Format [N] => N times confidence-score between 0 and 1)\n",
    "    labels: list of class-prediction (Format [N] => N times an number between 0 and _num_classes-1)\n",
    "    dataset: list of all classes e.g. [\"background\", \"class1\", \"class2\", ..., \"classN\"] => Format [N_classes]\n",
    "  '''\n",
    "\n",
    "  cmap = plt.get_cmap(\"tab20b\")\n",
    "  class_labels = np.array(dataset)\n",
    "  colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "  height, width, _ = img.shape\n",
    "  # Create figure and axes\n",
    "  fig, ax = plt.subplots(1, figsize=(16, 8))\n",
    "  # Display the image\n",
    "  ax.imshow(img)\n",
    "  for i, box in enumerate(boxes):\n",
    "    class_pred = labels[i]\n",
    "    conf = scores[i]\n",
    "    width = box[2] - box[0]\n",
    "    height = box[3] - box[1]\n",
    "    rect = patches.Rectangle(\n",
    "        (box[0], box[1]),\n",
    "        width,\n",
    "        height,\n",
    "        linewidth=2,\n",
    "        edgecolor=colors[int(class_pred)],\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "    plt.text(\n",
    "        box[0], box[1],\n",
    "        s=class_labels[int(class_pred)] + \" \" + str(int(100*conf)) + \"%\",\n",
    "        color=\"white\",\n",
    "        verticalalignment=\"top\",\n",
    "        bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
    "    )\n",
    "\n",
    "  # Used to save inference phase results\n",
    "  if save_path is not None:\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QrSrxQUkLvU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Prepping Door Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptraBrRVdG0a"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf62WlEVk2-W"
   },
   "source": [
    "### Download the CubiCasa5k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tS8KF8mUZsT_",
    "outputId": "2901cac5-f707-441b-ba25-6bb7338316ae"
   },
   "outputs": [],
   "source": [
    "!gdown --fuzzy https://drive.google.com/file/d/1Wm3q2vyEeFL-gGPEsbWVFX31rgtMyywp/view?usp=sharing -O cubicasa5k.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rahyRO5b4x2"
   },
   "outputs": [],
   "source": [
    "!tar -xf cubicasa5k.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d78QL-XMk88k"
   },
   "source": [
    "### Collate file of all items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRgp8vbUbLn1"
   },
   "source": [
    "Collect all samples from train, test and validation into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZY-ZNDpcWV8"
   },
   "outputs": [],
   "source": [
    "train = open(\"cubicasa5k/train.txt\", \"r\")\n",
    "test = open(\"cubicasa5k/test.txt\", \"r\")\n",
    "validation = open(\"cubicasa5k/val.txt\", \"r\")\n",
    "\n",
    "all = open(\"cubicasa5k/all.txt\", \"w\")\n",
    "all.write(train.read() + \"\\n\" + test.read() + \"\\n\" + validation.read() + \"\\n\")\n",
    "all.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXbNwkEibQf0"
   },
   "source": [
    "Here, we create a smaller image subset to be used for any debugging for the door bounding boxes extraction. The subset details are stored under the file \"cubicasa5k/sample.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a_2X4fJbP70"
   },
   "outputs": [],
   "source": [
    "file = open(\"cubicasa5k/all.txt\", \"r\")\n",
    "sample = open(\"cubicasa5k/sample.txt\", \"w\")\n",
    "\n",
    "section = file.readlines()[:10]\n",
    "sample.write(\"\".join(section))\n",
    "\n",
    "file.close()\n",
    "sample.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXAonn6ilClX"
   },
   "source": [
    "### Extract Door Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7515_TLbzix"
   },
   "source": [
    "Functions used to extract door bounding boxes from svg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkztjcYcdR-h"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_svg: string -> string\n",
    "REQUIRES: input string is a valid system path to an SVG file\n",
    "ENSURES: return value is the contents of the SVG pointed to by input path\n",
    "\"\"\"\n",
    "def get_svg(path):\n",
    "    file = open(path, \"r\")\n",
    "    svg = file.read()\n",
    "\n",
    "    file.close()\n",
    "    return svg\n",
    "\n",
    "\"\"\"\n",
    "get_door_tags: string -> bs4.element.ResultSet\n",
    "REQUIRES: string is XML of a SVG file\n",
    "ENSURES: returns a ResultSet of all tags with id = 'Door' inthe given SVG string\n",
    "\"\"\"\n",
    "def get_door_tags(svg):\n",
    "    soup = BeautifulSoup(svg, 'xml')\n",
    "    return soup.find_all(attrs={'id':'Door'})\n",
    "\n",
    "def str2coord(s):\n",
    "    (x, y) = s.split(\",\")\n",
    "    return int(float(x)), int(float(y))\n",
    "\n",
    "\"\"\"\n",
    "get_door_thresh: bs4.element -> string\n",
    "REQUIRES: given element has id = 'Door' and has a polygon tag with attribute\n",
    "          points, depicting a rectangle with coordinate arrangement bottom left,\n",
    "          bottom right, top right, top left\n",
    "ENSURES: returns a string of the first two coordinate pairs of the points\n",
    "         attribute, separated by a space\n",
    "\"\"\"\n",
    "def get_door_thresh(elem):\n",
    "    polygon = elem.find(\"polygon\")\n",
    "    coord = polygon.attrs[\"points\"].split(\" \")[:-1]\n",
    "    x = [str2coord(s)[0] for s in coord]\n",
    "    y = [str2coord(s)[1] for s in coord]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def get_curve_end(elem):\n",
    "    path = elem.find(\"path\")\n",
    "\n",
    "    coord = path.attrs[\"d\"].split(\" \")\n",
    "    coord = [s.strip(\" MmQqLl\\n\\r\\t\") for s in coord]\n",
    "\n",
    "    x = [str2coord(s)[0] for s in (coord[::2])]\n",
    "    y = [str2coord(s)[1] for s in (coord[::2])]\n",
    "\n",
    "    return sum(x), sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkaL7GIMcf13"
   },
   "source": [
    "Open file that has locations of floor plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l26w5v_sdTgY"
   },
   "outputs": [],
   "source": [
    "folder = \"cubicasa5k\"\n",
    "paths = open(\"cubicasa5k/all.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sm2D1xRcmNp"
   },
   "source": [
    "Go through each SVG one by one to get its bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcyGQSmhdtLF"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'Frame', 'Label'])\n",
    "\n",
    "# read each dataset path\n",
    "for line in paths:\n",
    "    path = line.strip() # path to output\n",
    "    svg = get_svg(folder + path + \"model.svg\")\n",
    "\n",
    "    door_elements = get_door_tags(svg)\n",
    "    i = 0\n",
    "\n",
    "    # write output value for each door\n",
    "    for elem in door_elements:\n",
    "        all_x, all_y = get_door_thresh(elem)\n",
    "        try:\n",
    "          x, y = get_curve_end(elem)\n",
    "        except:\n",
    "          continue\n",
    "\n",
    "        all_x.append(x)\n",
    "        all_y.append(y)\n",
    "\n",
    "        # print(min(all_x), min(all_y), max(all_x), max(all_y))\n",
    "        df.loc[len(df.index)] = [min(all_x), min(all_y), max(all_x), max(all_y),\n",
    "                                 path.strip(\"/\") + \"/F1_scaled.png\", \"Door\"]\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odsR9P58lIIi"
   },
   "source": [
    "### Save resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "gEEN3vkhf36X",
    "outputId": "297cd177-e767-432e-dce1-64fb444cf410"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Rhr6jfKcugX"
   },
   "source": [
    "Check all bounding boxes are valid and then save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZvlqT5TgNPE"
   },
   "outputs": [],
   "source": [
    "df = df.query(\"xmin < xmax and ymin < ymax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnvXF0CIgazH"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"doors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5DnF5vLBf7_"
   },
   "source": [
    "## 3. Finetuning of a pre-trained model with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT18SF0Lsp7l"
   },
   "source": [
    "Here, we finetune the pre-trained model from above. For this purpose, we use the CubiCasa5k floor plan images along with the bounding boxes extracted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YmXuOUiMJ8Pn"
   },
   "outputs": [],
   "source": [
    "path_csv = 'doors.csv'\n",
    "img_dir = 'cubicasa5k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo_LhlZrb5o-"
   },
   "source": [
    "### Dataset creation\n",
    "\n",
    "Use the default dataset of PyTorch to create the dataset class `imageDataset` with which the FasterRCNN can be trained. (https://pytorch.org/vision/main/models/faster_rcnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGJgTvdeH3JA"
   },
   "source": [
    "**Transformations (augmentations)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E1AiUUXVHqxg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Class that holds all the augmentation related attributes\n",
    "'''\n",
    "class Transformation():\n",
    "    # This provides a random probability of the augmentation to be applied or not\n",
    "    def get_probability(self):\n",
    "        return np.random.choice([False, True], replace=False, p=[0.5, 0.5])\n",
    "\n",
    "    # Increases the contrast by a factor of 2\n",
    "    def random_adjust_contrast(self, image, enable=None):\n",
    "        enable = self.get_probability() if enable is None else enable\n",
    "        return F.adjust_contrast(image, 2) if enable else image\n",
    "\n",
    "    # Increaes the brightness by a factor of 2\n",
    "    def random_adjust_brightness(self, image, enable=None):\n",
    "        enable = enable = self.get_probability() if enable is None else enable\n",
    "        return F.adjust_brightness(image,2) if enable else image\n",
    "\n",
    "    # Horizontal flip\n",
    "    def random_hflip(self, image, boxes, enable=None):\n",
    "        enable = enable = self.get_probability() if enable is None else enable\n",
    "        if enable:\n",
    "          #flip image\n",
    "          new_image = F.hflip(image)\n",
    "\n",
    "          #flip boxes\n",
    "          new_boxes = boxes.clone()\n",
    "          new_boxes[:, 0] = image.shape[2] - boxes[:, 0]  # image width - xmin\n",
    "          new_boxes[:, 2] = image.shape[2] - boxes[:, 2]  # image_width - xmax\n",
    "          new_boxes = new_boxes[:, [2, 1, 0, 3]]          # Interchange the xmin and xmax due to mirroring\n",
    "          return new_image, new_boxes\n",
    "        else:\n",
    "          return image, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTW7Sn3jH9kX"
   },
   "source": [
    "**Custom Dataset for Image Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FONOa9npqOY6"
   },
   "outputs": [],
   "source": [
    "class imageDataset(Dataset):\n",
    "  def __init__(self, img_path, label_path, classes, transforms=None):\n",
    "    super().__init__()\n",
    "    print(\"Preparing the dataset...\")\n",
    "\n",
    "    self.image_dir = img_path\n",
    "\n",
    "    self.gt_info = pd.read_csv(label_path)\n",
    "\n",
    "    self.classes = classes\n",
    "    self.transforms = transforms\n",
    "\n",
    "    # Create a list of image file names (in sorted order - this is optional)\n",
    "    self.image_paths = [f\"{img_path}{row['Frame']}\" for i, row in self.gt_info.iterrows()]\n",
    "    self.all_images = list(self.gt_info['Frame'])\n",
    "\n",
    "    # Map Label (str) --> Label (int)\n",
    "    for i in range(len(self.gt_info)):\n",
    "      label = self.gt_info.loc[i, 'Label']\n",
    "      self.gt_info.loc[i, 'Label'] = self.classes.index(label)\n",
    "\n",
    "    # Filter the dataset based on given conditions:\n",
    "    self.filter_dataset()\n",
    "    print(\"Dataset prepared\")\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    target = {}\n",
    "\n",
    "    # Read input image\n",
    "    image_name = self.all_images[idx]\n",
    "    image_path = self.image_paths[idx]\n",
    "    image = cv2.imread(image_path)\n",
    "    try:\n",
    "      image = img_transform(image)\n",
    "    except:\n",
    "      raise Exception(f\"libpng error found on image name: {image_name}, image path: {image_path}\\n\")\n",
    "\n",
    "    # Fetch GT infos for given image\n",
    "    gt_info = self.gt_info[self.gt_info['Frame'] == image_name]\n",
    "\n",
    "    boxes = torch.Tensor(gt_info[['xmin', 'ymin', 'xmax', 'ymax']].values.astype('float')).float()\n",
    "    labels = torch.LongTensor(gt_info['Label'].values.tolist())\n",
    "\n",
    "    if self.transforms:\n",
    "        image = self.transforms.random_adjust_contrast(image, enable=True)\n",
    "        image = self.transforms.random_adjust_brightness(image, enable=True)\n",
    "        image, boxes = self.transforms.random_hflip(image, boxes, enable=True)\n",
    "\n",
    "    target[\"boxes\"] = boxes     # Hint: Shape -> [N, 4] with N = Number of Boxes\n",
    "    target[\"labels\"] = labels   # Hint: Shape -> [N] with N = Number of Boxes\n",
    "\n",
    "    return image, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.all_images)\n",
    "\n",
    "  '''\n",
    "  Filter the dataset by removing images with no labels and incorrect bounding boxes\n",
    "  '''\n",
    "  def filter_dataset(self):\n",
    "    print(\"Filtering the dataset...\")\n",
    "    remove_images = []\n",
    "\n",
    "    # There are no labels for some images because they show an 'empty' scene → these images should be filtered out.\n",
    "    for image_file in self.all_images.copy():\n",
    "      if image_file not in self.gt_info['Frame'].values:\n",
    "        remove_images.append(image_file)\n",
    "        self.all_images.remove(image_file)\n",
    "    print(\"Images removed with no labels: \", len(remove_images))\n",
    "\n",
    "    self.gt_info = self.gt_info.query(\"xmin < xmax and ymin < ymax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKQkyGmiqOY6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the created dataset class by running an infernce on random images of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c17DyxMiqOY6",
    "outputId": "186c9f7f-feac-46f5-c726-a19db705b558"
   },
   "outputs": [],
   "source": [
    "# crowdai = [\"Background\",\"Door\"]\n",
    "crowdai = [\"Background\", \"Car\", \"Truck\", \"Pedestrian\"]\n",
    "dataset = imageDataset(img_dir, path_csv, crowdai, transforms=Transformation()) # here your arguments\n",
    "num_images = 5\n",
    "\n",
    "for i in range(num_images):\n",
    "  x = random.randint(0, (dataset.__len__()-1))\n",
    "  img, target = dataset.__getitem__(x)\n",
    "  img = img.cpu().permute(1,2,0).numpy()\n",
    "  boxes = target['boxes'].numpy()\n",
    "  labels = target['labels'].numpy()\n",
    "  scores = [1]*len(labels)\n",
    "  print(\"Image index: \", x)\n",
    "  plot_image(img, boxes, scores, labels, crowdai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.gt_info.iloc[2365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = dataset.__getitem__(2365)\n",
    "img = img.cpu().permute(1,2,0).numpy()\n",
    "boxes = target['boxes'].numpy()\n",
    "labels = target['labels'].numpy()\n",
    "scores = [1]*len(labels)\n",
    "print(\"Image index: \", 2365)\n",
    "plot_image(img, boxes, scores, labels, crowdai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me5fCaUU0KeR"
   },
   "source": [
    "### Fine-Tune\n",
    "\n",
    "The Faster RCNN model is now to be trained for 5 epochs on the new dataset. Several tasks have to be done for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e4XoE9HUlKPY"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "NUM_EPOCHS = 34\n",
    "\n",
    "LR = 0.005\n",
    "LR_MOMENTUM=0.9\n",
    "LR_DECAY_RATE=0.0005\n",
    "\n",
    "LR_SCHED_STEP_SIZE = 5\n",
    "LR_SCHED_GAMMA = 0.1\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_TEST_IMAGES = 5\n",
    "NMS_THRESH = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbwP183ZlKPY"
   },
   "source": [
    "Create an output directory to store checkpoints and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nczBAe6AlKPc"
   },
   "outputs": [],
   "source": [
    "current_dir = \"\"\n",
    "\n",
    "# Fetch current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y\")\n",
    "output_dir_name = \"output-\" + dt_string\n",
    "\n",
    "OUTPUT_DIR = os.path.join(current_dir, output_dir_name)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIN-ti_7qOY7",
    "outputId": "52ccb340-3004-42ac-dcb7-81a3208c1b42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hudab/myconda/miniconda3/envs/doors_colab/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the dataset...\n",
      "Filtering the dataset...\n",
      "Images removed with no labels:  0\n",
      "Dataset prepared\n",
      "Preparing the dataset...\n",
      "Filtering the dataset...\n",
      "Images removed with no labels:  0\n",
      "Dataset prepared\n"
     ]
    }
   ],
   "source": [
    "crowdai = [\"Background\", \"Door\"]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(min_size=300, max_size=480, weights=True)\n",
    "num_classes = 2  # 1 classes + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LR, momentum=LR_MOMENTUM, weight_decay=LR_DECAY_RATE)\n",
    "\n",
    "# create a train- and validation-dataset with our imageDataset\n",
    "# split the dataset in train and test set\n",
    "dataset = imageDataset(img_dir, path_csv, crowdai, transforms=Transformation())      # here your arguments\n",
    "dataset_test = imageDataset(img_dir, path_csv, crowdai, transforms=None)             # here your arguments\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "\n",
    "test_size = int(len(dataset) * TEST_SIZE)\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-test_size])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-test_size:])\n",
    "\n",
    "# create a learning rate scheduler\n",
    "# TODO: step size to be tuned !\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \\\n",
    "                                                step_size=LR_SCHED_STEP_SIZE, \\\n",
    "                                                gamma=LR_SCHED_GAMMA)\n",
    "\n",
    "def collate_fn(batch):\n",
    "  return tuple(zip(*batch))\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,\n",
    "  collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
    "  collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317 9265\n"
     ]
    }
   ],
   "source": [
    "print(len(data_loader), len(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37063 9265\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYs9ZU06lKPd"
   },
   "source": [
    "Train per epoch, validation loop and plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JPSWhT7xlKPd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to train the model over one epoch.\n",
    "'''\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "  train_loss_list = []\n",
    "\n",
    "  tqdm_bar = tqdm(data_loader, total=len(data_loader))\n",
    "  for idx, data in enumerate(tqdm_bar):\n",
    "    optimizer.zero_grad()\n",
    "    images, targets = data\n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # targets = {'boxes'=tensor, 'labels'=tensor}\n",
    "\n",
    "    losses = model(images, targets)\n",
    "\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss_val = loss.item()\n",
    "    train_loss_list.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    tqdm_bar.set_description(desc=f\"Training Loss: {loss:.3f}\")\n",
    "\n",
    "  return train_loss_list\n",
    "\n",
    "'''\n",
    "Function to validate the model\n",
    "'''\n",
    "def evaluate(model, data_loader_test, device, m_1, m_5):\n",
    "    val_loss_list = []\n",
    "\n",
    "    target = dict(boxes=torch.Tensor(),labels=torch.Tensor(),)\n",
    "    preds = dict(boxes=torch.Tensor(),scores=torch.Tensor(),labels=torch.Tensor(),)\n",
    "    \n",
    "    tqdm_bar = tqdm(data_loader_test, total=len(data_loader_test))\n",
    "    for i, data in enumerate(tqdm_bar):\n",
    "        imges, tgt = data\n",
    "\n",
    "        images = list(image.to(device) for image in imges)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in tgt]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            losses = model(images, targets)\n",
    "\n",
    "        loss = sum(loss for loss in losses.values())\n",
    "        loss_val = loss.item()\n",
    "        val_loss_list.append(loss_val)\n",
    "        \n",
    "        box, score, label = inference(imges[0], model)\n",
    "        model.train()\n",
    "\n",
    "        target = [dict(boxes=tgt[0][\"boxes\"].to(\"cpu\"),labels=tgt[0][\"labels\"].to(\"cpu\"),)]\n",
    "        preds = [dict(boxes=torch.tensor(box),scores=torch.tensor(score),labels=torch.tensor(label),)]\n",
    "        \n",
    "        tqdm_bar.set_description(desc=f\"Validation Loss: {loss:.4f}\")\n",
    "        \n",
    "        m_1.update(preds, target)\n",
    "        m_5.update(preds, target)\n",
    "    return val_loss_list\n",
    "\n",
    "'''\n",
    "Function to plot training and valdiation losses and save them in `output_dir'\n",
    "'''\n",
    "def plot_loss(train_loss, valid_loss):\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "\n",
    "    train_ax.plot(train_loss, color='blue')\n",
    "    train_ax.set_xlabel('Iteration')\n",
    "    train_ax.set_ylabel('Training Loss')\n",
    "\n",
    "    valid_ax.plot(valid_loss, color='red')\n",
    "    valid_ax.set_xlabel('Iteration')\n",
    "    valid_ax.set_ylabel('Validation loss')\n",
    "\n",
    "    figure_1.savefig(f\"{OUTPUT_DIR}/train_loss.png\")\n",
    "    figure_2.savefig(f\"{OUTPUT_DIR}/valid_loss.png\")\n",
    "\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output-25-03-2024'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_custom = [i / 100 for i in range(1, 11, 1)]\n",
    "iou_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJiS4YFElKPe"
   },
   "source": [
    "Main train loop over all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChLl6ysxlKPe",
    "outputId": "d8b8e5e3-a499-4761-c1a9-06a4a359de10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint found!\n",
      "----------Epoch 33----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.098: 100%|█████████████████████████████████████████| 2317/2317 [1:00:20<00:00,  1.56s/it]\n",
      "Validation Loss: 0.2902: 100%|████████████████████████████████████████| 9265/9265 [12:45<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@[0.01:0.1]: \n",
      "{'classes': 1,\n",
      " 'map': 0.7967436909675598,\n",
      " 'map_50': -1.0,\n",
      " 'map_75': -1.0,\n",
      " 'map_large': 0.7767582535743713,\n",
      " 'map_medium': 0.805787205696106,\n",
      " 'map_per_class': -1.0,\n",
      " 'map_small': 0.1382978856563568,\n",
      " 'mar_1': 0.07661794871091843,\n",
      " 'mar_10': 0.6348279714584351,\n",
      " 'mar_100': 0.8271962404251099,\n",
      " 'mar_100_per_class': -1.0,\n",
      " 'mar_large': 0.8187423944473267,\n",
      " 'mar_medium': 0.8337820172309875,\n",
      " 'mar_small': 0.188043475151062}\n",
      "mAP@[0.5:0.95]: \n",
      "{'classes': 1,\n",
      " 'map': 0.4703085422515869,\n",
      " 'map_50': 0.7478157877922058,\n",
      " 'map_75': 0.5338945984840393,\n",
      " 'map_large': 0.43276548385620117,\n",
      " 'map_medium': 0.4828597605228424,\n",
      " 'map_per_class': -1.0,\n",
      " 'map_small': 0.0,\n",
      " 'mar_1': 0.05661710724234581,\n",
      " 'mar_10': 0.44582992792129517,\n",
      " 'mar_100': 0.5507619380950928,\n",
      " 'mar_100_per_class': -1.0,\n",
      " 'mar_large': 0.5346775054931641,\n",
      " 'mar_medium': 0.5555819272994995,\n",
      " 'mar_small': 0.0}\n",
      "----------Epoch 34----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.103: 100%|███████████████████████████████████████████| 2317/2317 [51:08<00:00,  1.32s/it]\n",
      "Validation Loss: 0.3034: 100%|████████████████████████████████████████| 9265/9265 [12:24<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP@[0.01:0.1]: \n",
      "{'classes': 1,\n",
      " 'map': 0.7967552542686462,\n",
      " 'map_50': -1.0,\n",
      " 'map_75': -1.0,\n",
      " 'map_large': 0.7767672538757324,\n",
      " 'map_medium': 0.8058081269264221,\n",
      " 'map_per_class': -1.0,\n",
      " 'map_small': 0.1382978856563568,\n",
      " 'mar_1': 0.07661794871091843,\n",
      " 'mar_10': 0.6348220705986023,\n",
      " 'mar_100': 0.8271549344062805,\n",
      " 'mar_100_per_class': -1.0,\n",
      " 'mar_large': 0.8187423944473267,\n",
      " 'mar_medium': 0.8337299227714539,\n",
      " 'mar_small': 0.188043475151062}\n",
      "mAP@[0.5:0.95]: \n",
      "{'classes': 1,\n",
      " 'map': 0.4702816903591156,\n",
      " 'map_50': 0.7478196620941162,\n",
      " 'map_75': 0.5338643193244934,\n",
      " 'map_large': 0.4327622652053833,\n",
      " 'map_medium': 0.4828207790851593,\n",
      " 'map_per_class': -1.0,\n",
      " 'map_small': 0.0,\n",
      " 'mar_1': 0.05662974342703819,\n",
      " 'mar_10': 0.4458189904689789,\n",
      " 'mar_100': 0.5507248640060425,\n",
      " 'mar_100_per_class': -1.0,\n",
      " 'mar_large': 0.5346531271934509,\n",
      " 'mar_medium': 0.555541455745697,\n",
      " 'mar_small': 0.0}\n",
      "Training Finished !\n"
     ]
    }
   ],
   "source": [
    "# find latest saved chcekpoint\n",
    "checkpoint_files = [f for f in os.listdir(f\"/home/hudab/door_model/output-16-03-2024/\") if f.endswith('.pth')]\n",
    "if checkpoint_files:\n",
    "    # Last ckpt file\n",
    "    print(\"checkpoint found!\")\n",
    "    checkpoint_files.sort()\n",
    "    latest_checkpoint = os.path.join(OUTPUT_DIR, checkpoint_files[-1])\n",
    "\n",
    "    # Load the ckpt\n",
    "    checkpoint = torch.load(f\"/home/hudab/door_model/output-16-03-2024/epoch_32_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    loss_dict = checkpoint['loss_dict']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    loss_dict = {'train_loss': [], 'valid_loss': []}\n",
    "\n",
    "'''\n",
    "Train the model over all epochs\n",
    "'''\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    m_1 = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=iou_custom)\n",
    "    m_5 = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    print(\"----------Epoch {}----------\".format(epoch+1))\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    train_loss_list = train_one_epoch(model, optimizer, data_loader, device, epoch)\n",
    "    loss_dict['train_loss'].extend(train_loss_list)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Run evaluation\n",
    "    valid_loss_list = evaluate(model, data_loader_test, device, m_1, m_5)\n",
    "    loss_dict['valid_loss'].extend(valid_loss_list)\n",
    "    \n",
    "    # Svae the model ckpt after every epoch\n",
    "    ckpt_file_name = f\"{OUTPUT_DIR}/epoch_{epoch+1}_model.pth\"\n",
    "    torch.save({\n",
    "    'epoch': epoch+1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_dict': loss_dict\n",
    "    }, ckpt_file_name)\n",
    "    \n",
    "    # NOTE: The losses are accumulated over all iterations\n",
    "    plot_loss(loss_dict['train_loss'], loss_dict['valid_loss'])\n",
    "    map1 = m_1.compute()\n",
    "    map2 = m_5.compute()\n",
    "\n",
    "    map1 = {k: (v.item()) for k, v in map1.items()}\n",
    "    map2 = {k: (v.item()) for k, v in map2.items()}\n",
    "\n",
    "    print(\"mAP@[0.01:0.1]: \")\n",
    "    pprint(map1)\n",
    "    print(\"mAP@[0.5:0.95]: \")\n",
    "    pprint(map2)\n",
    "\n",
    "# Store the losses after the training in a pickle\n",
    "with open(f\"loss_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(loss_dict, file)\n",
    "\n",
    "print(\"Training Finished !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5msa6TEqOY7"
   },
   "source": [
    "### Test Fine-Tuned Model\n",
    "\n",
    "Test the finetuned FasterRCNN by running an infernce on random images of the validation-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load last checkpoint\n",
    "# CHANGE THE OUTPUT_DIR IF CKPT IS STORED ELSEWHERE\n",
    "checkpoint_dir = f\"output-16-03-2024/epoch_32_model.pth\"\n",
    "checkpoint = torch.load(checkpoint_dir, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Kmi6keHRu1Ak",
    "outputId": "ba1ffe0a-cf6c-4c18-d97b-935a742c3925"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "crowdai = [\"Background\", \"Door\"]\n",
    "num_images = NUM_TEST_IMAGES\n",
    "\n",
    "for _ in range(num_images):\n",
    "  x = random.randint(0, (dataset_test.__len__()-1))\n",
    "  img, target = dataset_test.dataset.__getitem__(x)\n",
    "  img = img.to(device)\n",
    "\n",
    "  # Load last checkpoint\n",
    "  # CHANGE THE OUTPUT_DIR IF CKPT IS STORED ELSEWHERE\n",
    "  checkpoint_dir = f\"output-16-03-2024/epoch_32_model.pth\"\n",
    "  checkpoint = torch.load(checkpoint_dir, map_location=device)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "  print(dataset_test.dataset.gt_info.iloc[x])\n",
    "\n",
    "  boxes, scores, labels = inference(img, model)\n",
    "  # print(target, boxes, scores, labels)\n",
    "\n",
    "  img = img.squeeze(0).cpu().permute(1,2,0).numpy()\n",
    "  plot_image(img, boxes, scores, labels, crowdai, save_path=f\"inference_{x}.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "z5msa6TEqOY7"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "91745af26735e2fb8e91cfbd0927addffd49b8ef53a4792a6e721c1d45b56778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
